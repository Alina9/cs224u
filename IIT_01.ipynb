{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from iit import get_equality_dataset, get_IIT_equality_dataset, get_IIT_equality_dataset_both\n",
    "from torch_deep_neural_classifier_iit import TorchDeepNeuralClassifierIIT, TorchDeepNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Hierarchical Equality Dataset](#Hierarchical-Equality-Dataset)\n",
    "2. [High-Level Tree-Structured Algorithm](#The-High-Level-Tree-Structured-Algorithm)\n",
    "3. [A Fully-Connected Feed-Forward Neural Network](#A-Fully-Connected-Feed-Forward-Neural-Network)\n",
    "4. [Causal Abstraction](#Causal-Abstraction)\n",
    "5. [Interchange Intervention Training (IIT)](#Interchange-Intervention-Training-(IIT))\n",
    "\n",
    "## Hierarchical Equality Dataset  \n",
    "[Geiger, Carstensen, Frank, and Potts (2020)](https://arxiv.org/abs/2006.07968)\n",
    "\n",
    "We will use a hierarchical equality task to present IIT. We define the hierarchical equality task as follows: The input is two pairs of objects and the output is **true** if both pairs contain the same object or if both pairs contain different objects and **false** otherwise. For example, AABB and ABCD are both labeled **true** while ABCC and BBCD are both labeled **false**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The High-Level Tree-Structured Algorithm\n",
    "\n",
    "Let $\\mathcal{A}$ be the simple tree structured algorithm that solves this task by applying a simple equality relation three times: Compute whether the first two inputs are equal, compute whether the second two inputs are equal, then compute whether\n",
    "the truth-valued outputs of these first two computations are equal. We visually define $\\mathcal{A}$ below and then define a python function that computes $\\mathcal{A}$, possibly under an intervention that sets $V_1$ and/or $V_2$ to fixed values.\n",
    "\n",
    "<img src=\"fig/IIT/PremackFunctions.png\" width=\"500\"/>\n",
    "<img src=\"fig/IIT/PremackGraph.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_A(input, intervention):\n",
    "    graph = dict()\n",
    "    for i, object in enumerate(input):\n",
    "        graph[\"input\" + str(i+1)] = object\n",
    "    if \"V1\" in intervention:\n",
    "        graph[\"V1\"] = intervention[\"V1\"]\n",
    "    else:\n",
    "        graph[\"V1\"] = graph[\"input1\"] == graph[\"input2\"]\n",
    "    if \"V2\" in intervention:\n",
    "        graph[\"V2\"] = intervention[\"V2\"]\n",
    "    else:\n",
    "        graph[\"V2\"] = graph[\"input3\"] == graph[\"input4\"]\n",
    "    graph[\"output\"] = graph[\"V1\"] == graph[\"V2\"]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The algorithm with no intervention\n",
    "\n",
    "First, observe the behavior of the algorithm whhen we provide the input **(pentagon,pentagon, triangle, square)** with no intervention. We show this visually and by using our **compute_A** function.\n",
    "\n",
    "<img src=\"fig/IIT/PremackNoIntervention.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input1': 'pentagon',\n",
       " 'input2': 'pentagon',\n",
       " 'input3': 'triangle',\n",
       " 'input4': 'square',\n",
       " 'V1': True,\n",
       " 'V2': False,\n",
       " 'output': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_A((\"pentagon\", \"pentagon\", \"triangle\", \"square\"), {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The algorithm with an intervention\n",
    "\n",
    "Observe the behavior of the algorithm whhen we provide the input **(square,pentagon, triangle, triangle)** with an intervention setting **V1** to **False**. We show this visually and by using our **compute_A** function.\n",
    "\n",
    "<img src=\"fig/IIT/PremackIntervention.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input1': 'square',\n",
       " 'input2': 'pentagon',\n",
       " 'input3': 'triangle',\n",
       " 'input4': 'triangle',\n",
       " 'V1': True,\n",
       " 'V2': True,\n",
       " 'output': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_A((\"square\", \"pentagon\", \"triangle\", \"triangle\"), {\"V1\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The algorithm with an interchange intervention\n",
    "\n",
    "Finaally, observe the behavior of the algorithm when we provide the base input **(square,pentagon, triangle, triangle)** with an intervention setting **V1** to be the value it would be for the source input **(pentagon,pentagon, triangle, square)**. We show this visually and by using our **compute_A** function.\n",
    "\n",
    "<img src=\"fig/IIT/algorithmII.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input1': 'pentagon',\n",
       " 'input2': 'pentagon',\n",
       " 'input3': 'triangle',\n",
       " 'input4': 'square',\n",
       " 'V1': False,\n",
       " 'V2': False,\n",
       " 'output': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_interchange_A(base,source, variable):\n",
    "    return compute_A(base, {variable:compute_A(source, {})[variable]})\n",
    "    \n",
    "compute_interchange_A((\"pentagon\", \"pentagon\", \"triangle\", \"square\"), (\"square\", \"pentagon\", \"triangle\", \"triangle\"), \"V1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Fully-Connected Feed-Forward Neural Network\n",
    "\n",
    "We will train a three layer feed-forward neural network on this task where each object has a random vector assigned to it and the objects in training are disjoint from the objects seen in testing.\n",
    "\n",
    "<img src=\"fig/IIT/Network.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterventionableTorchDeepNeuralClassifier(TorchDeepNeuralClassifier):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def make_hook(self, gets, sets, layer):\n",
    "        def hook(model, input, output):\n",
    "            layer_gets, layer_sets = [], []\n",
    "            if gets is not None and layer in gets:\n",
    "                layer_gets = gets[layer]\n",
    "            if sets is not None and layer in sets:\n",
    "                layer_sets = sets[layer]\n",
    "            for get in layer_gets:\n",
    "                self.activation[f'{get[\"layer\"]}-{get[\"start\"]}-{get[\"end\"]}'] = output[:,get[\"start\"]: get[\"end\"] ]\n",
    "            for set in layer_sets:\n",
    "                output[:,set[\"start\"]: set[\"end\"]] = set[\"intervention\"]\n",
    "        return hook\n",
    "\n",
    "    def _gets_sets(self,gets=None, sets = None):\n",
    "        handlers = []\n",
    "        for layer in range(len(self.layers)):\n",
    "            hook = self.make_hook(gets,sets, layer)\n",
    "            both_handler = self.layers[layer].register_forward_hook(hook)\n",
    "            handlers.append(both_handler)\n",
    "        return handlers\n",
    "\n",
    "    def retrieve_activations(self, input, get, sets):\n",
    "        input = input.type(torch.FloatTensor).to(self.device)\n",
    "        self.activation = dict()\n",
    "        handlers = self._gets_sets({get[\"layer\"]:[get]}, sets)\n",
    "        logits = self.model(input)\n",
    "        for handler in handlers:\n",
    "            handler.remove()\n",
    "        return self.activation[f'{get[\"layer\"]}-{get[\"start\"]}-{get[\"end\"]}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 484. Training loss did not improve more than tol=1e-05. Final error is 0.0014873843174427748."
     ]
    }
   ],
   "source": [
    "\n",
    "TRUE_LABEL = 1\n",
    "FALSE_LABEL = 0\n",
    "\n",
    "data_size = 1024 * 10\n",
    "embedding_dim = 4\n",
    "X_train, X_test, y_train, y_test, test_dataset = get_equality_dataset(embedding_dim,100)\n",
    "\n",
    "model = InterventionableTorchDeepNeuralClassifier(hidden_dim=4*embedding_dim, hidden_activation=torch.nn.ReLU(), num_layers=3)\n",
    "_ = model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this neural network achieves near perfect performance on its test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0306, -0.0595, -0.3156, -0.4486, -0.0306, -0.0595, -0.3156, -0.4486,\n",
      "         0.4411, -0.0223,  0.3221, -0.0993, -0.4259,  0.1294, -0.4464, -0.3508],\n",
      "       dtype=torch.float64) 0\n",
      "Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.54      0.55        50\n",
      "           1       0.55      0.56      0.55        50\n",
      "\n",
      "    accuracy                           0.55       100\n",
      "   macro avg       0.55      0.55      0.55       100\n",
      "weighted avg       0.55      0.55      0.55       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0], y_train[0])\n",
    "preds = model.predict(X_train)\n",
    "print(\"Train Results\")\n",
    "print(classification_report(y_train, preds))\n",
    "preds = model.predict(X_test)\n",
    "print(\"\\n\\n\\nTest Results\")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network with no intervention\n",
    "\n",
    "First, observe the behavior of the network when we provide the input **(pentagon,pentagon, triangle, square)** with no intervention. We assign each shape a random vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[0.22598836347044526, 0.2552321570675361, -0.07076905924465338, -0.40880081278310687, 0.22598836347044526, 0.2552321570675361, -0.07076905924465338, -0.40880081278310687, -0.4048163220205434, -0.2865788230537869, -0.4220612784273938, 0.09367949684106447, 0.019444254636635794, -0.05493312322134569, -0.4919374309808232, -0.1564258499658926]]\n",
      "\n",
      "Layer 0: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.1312, 0.4781, 0.0000, 0.4358, 0.0000, 0.1913, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4050, 0.5134, 1.0772, 0.7328, 0.1617]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 1: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.7966, 0.0000, 0.4038, 1.3004, 0.6069, 0.5085, 0.8640, 1.0712, 0.3403,\n",
      "         0.4802, 0.0998, 0.0000, 0.3822, 0.2907, 0.8604, 0.3601]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 2: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.0000, 2.4316, 0.0000, 0.0000, 0.0000, 1.9469, 2.3879, 2.5806, 0.0000,\n",
      "         1.9361, 0.0000, 0.0000, 1.8707, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 3: Linear(in_features=16, out_features=2, bias=True)\n",
      "\n",
      "Neural Activations: tensor([[-5.9835,  4.9851]], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "pentagon = [random.uniform(-0.5,0.5) for _ in range(embedding_dim)]\n",
    "triangle = [random.uniform(-0.5,0.5) for _ in range(embedding_dim)]\n",
    "square = [random.uniform(-0.5,0.5) for _ in range(embedding_dim)]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Input:\",[[*pentagon,*pentagon,*triangle,*square]])\n",
    "for k in range(len(model.layers)):\n",
    "    get_coord = {\"layer\":k, \"start\":0, \"end\":embedding_dim*4}\n",
    "    print(f\"\\nLayer {k}:\", model.layers[k])\n",
    "    print(\"\\nNeural Activations:\", model.retrieve_activations(torch.tensor([[*pentagon,*pentagon,*triangle,*square]]), get_coord, None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network with an intervention\n",
    "\n",
    "Now, observe the behavior of the network when we provide the input **(pentagon,pentagon, triangle, square)** with an intervention that zeros out five neurons after the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[0.22598836347044526, 0.2552321570675361, -0.07076905924465338, -0.40880081278310687, 0.22598836347044526, 0.2552321570675361, -0.07076905924465338, -0.40880081278310687, -0.4048163220205434, -0.2865788230537869, -0.4220612784273938, 0.09367949684106447, 0.019444254636635794, -0.05493312322134569, -0.4919374309808232, -0.1564258499658926]]\n",
      "\n",
      "Layer 0: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.1312, 0.4781, 0.0000, 0.4358, 0.0000, 0.1913, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4050, 0.5134, 1.0772, 0.7328, 0.1617]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 1: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.7966, 0.0000, 0.4038, 1.3004, 0.6069, 0.5085, 0.8640, 1.0712, 0.3403,\n",
      "         0.4802, 0.0998, 0.0000, 0.3822, 0.2907, 0.8604, 0.3601]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 2: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.0000, 2.4316, 0.0000, 0.0000, 0.0000, 1.9469, 2.3879, 2.5806, 0.0000,\n",
      "         1.9361, 0.0000, 0.0000, 1.8707, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 3: Linear(in_features=16, out_features=2, bias=True)\n",
      "\n",
      "Neural Activations: tensor([[-5.9835,  4.9851]], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "set_coord = {\"layer\":1, \"start\":0, \"end\":embedding_dim, \"intervention\": torch.tensor([[0 for _ in range(embedding_dim)]])}\n",
    "\n",
    "print(\"Input:\",[[*pentagon,*pentagon,*triangle,*square]])\n",
    "for k in range(len(model.layers)):\n",
    "    get_coord = {\"layer\":k, \"start\":0, \"end\":embedding_dim*4}\n",
    "    print(f\"\\nLayer {k}:\", model.layers[k])\n",
    "    print(\"\\nNeural Activations:\", model.retrieve_activations(torch.tensor([[*pentagon,*pentagon,*triangle,*square]]), get_coord, set_coord))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network with an interchange intervention\n",
    "\n",
    "Finally, observe the behavior of the network when we provide the input **(square, pentagon, triangle, triangle)** with an intervention that sets five neurons after the first hidden layer to the values they achieve for the source input **(pentagon,pentagon, triangle, square)**.\n",
    "<img src=\"fig/IIT/networkII.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[0.22598836347044526, 0.2552321570675361, -0.07076905924465338, -0.40880081278310687, 0.22598836347044526, 0.2552321570675361, -0.07076905924465338, -0.40880081278310687, -0.4048163220205434, -0.2865788230537869, -0.4220612784273938, 0.09367949684106447, 0.019444254636635794, -0.05493312322134569, -0.4919374309808232, -0.1564258499658926]]\n",
      "\n",
      "Layer 0: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.0000, 0.5111, 0.0000, 0.4507, 0.0000, 0.0280, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1421, 0.4130, 0.0450, 0.6898, 0.6195, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 1: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.4637, 0.1112, 0.7164, 0.9098, 0.2960, 0.6310, 0.4019, 0.6862, 0.5158,\n",
      "         0.3605, 0.3475, 0.0000, 0.2031, 0.6713, 0.4579, 0.2025]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 2: ActivationLayer(\n",
      "  (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "Neural Activations: tensor([[0.3187, 0.9003, 0.5352, 0.4164, 0.3309, 0.6498, 1.0861, 0.8637, 0.6230,\n",
      "         0.7064, 0.5944, 0.5729, 0.6244, 0.3775, 0.0000, 0.4286]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "\n",
      "Layer 3: Linear(in_features=16, out_features=2, bias=True)\n",
      "\n",
      "Neural Activations: tensor([[-0.2492,  0.1612]], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "intervention = model.retrieve_activations(torch.tensor([[*pentagon,*pentagon,*triangle,*square]]), {\"layer\":1, \"start\":0, \"end\":embedding_dim},None)\n",
    "\n",
    "set_coord = {\"layer\":1, \"start\":0, \"end\":embedding_dim, \"intervention\": intervention}\n",
    "\n",
    "print(\"Input:\",[[*pentagon,*pentagon,*triangle,*square]])\n",
    "for k in range(len(model.layers)):\n",
    "    get_coord = {\"layer\":k, \"start\":0, \"end\":embedding_dim*4}\n",
    "    print(f\"\\nLayer {k}:\", model.layers[k])\n",
    "    print(\"\\nNeural Activations:\", model.retrieve_activations(torch.tensor([[*square,*pentagon,*triangle,*triangle]]), get_coord, set_coord))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Abstraction\n",
    "\n",
    "We defined a **high-level tree structured agorithm** that solves the hierarchical equality task.\n",
    "\n",
    "We trained a **low-level fully connected neural network** that solves the hierarchical equality task.\n",
    "\n",
    "A formal theory of **causal abstraction** describes the conditions that must hold for the high-level tree structured algorithm to be a **simplified and faithful description** of the neural network: \n",
    "\n",
    "**An algorithm is a causal abstraction of a neural network if and only if for all base and source inputs, the algorithm and network provides the same output under an aligned interchange intervention.**\n",
    "\n",
    "Below, we define an alignment between the neural network and the algorithm and a function to compute the **interchange intervention training accuracy** for a high-level variable, which is the percentage of aligned interchange interventions that the network and algorithm produce the same output on. When the IIT accuracy is 100%, the causal abstraction relation holds between the network and a simplified version of the algorithm where only one high-level variable exists.\n",
    "\n",
    "<img src=\"fig/IIT/alignment.png\" width=\"500\"/>\n",
    "\n",
    "We compute the IIT accuracy on our toy domain where each entity is either a pentagon, square, or triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "alignment = {\"V1\": {\"layer\":1, \"start\":0, \"end\":embedding_dim}, \"V2\": {\"layer\":1, \"start\":embedding_dim, \"end\":embedding_dim*2}}\n",
    "\n",
    "def interchange_intervention(model, base, source, int_coord, output_coord):\n",
    "    intervention = model.retrieve_activations(source, int_coord[1][0],None)\n",
    "    int_coord[1][0][\"intervention\"] = intervention\n",
    "    return model.retrieve_activations(base, output_coord, int_coord)\n",
    "\n",
    "def convert_input(tensor, embedding_dim):\n",
    "    return [tuple(tensor[0,embedding_dim*k:embedding_dim*(k+1)].flatten().tolist()) for k in range(4)]\n",
    "\n",
    "def compute_IIT_accuracy(variable, model):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for base in itertools.product([pentagon, triangle, square], repeat=4):\n",
    "        for source in itertools.product([pentagon, triangle, square], repeat=4):\n",
    "            basetensor = torch.cat([torch.tensor([base[k]]) for k in range(4)], 1)\n",
    "            sourcetensor = torch.cat([torch.tensor([source[k]]) for k in range(4)],1)\n",
    "            algorithm_output = compute_interchange_A(convert_input(basetensor, embedding_dim), convert_input(sourcetensor, embedding_dim), variable)\n",
    "            if algorithm_output[\"output\"]:   \n",
    "                labels.append(TRUE_LABEL)\n",
    "            else:\n",
    "                labels.append(FALSE_LABEL)\n",
    "            output_coord = {\"layer\":3, \"start\":0, \"end\":2}\n",
    "            network_output = interchange_intervention(model, basetensor, sourcetensor,{1:[copy.deepcopy(alignment[variable])]}, output_coord).argmax(axis=1)\n",
    "            predictions.append(int(network_output))\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we have low IIT accuracy for both **V1** and **V2**, meaning that under this alignment the neural network does not compute either variable. We have no evidence that this network computes simple equality relations to solve this hierarchical equality task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.31      0.36      2916\n",
      "           1       0.56      0.70      0.62      3645\n",
      "\n",
      "    accuracy                           0.52      6561\n",
      "   macro avg       0.50      0.50      0.49      6561\n",
      "weighted avg       0.51      0.52      0.50      6561\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.30      0.36      2916\n",
      "           1       0.56      0.70      0.62      3645\n",
      "\n",
      "    accuracy                           0.52      6561\n",
      "   macro avg       0.50      0.50      0.49      6561\n",
      "weighted avg       0.51      0.52      0.50      6561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(*compute_IIT_accuracy(\"V1\", model)))\n",
    "print(classification_report(*compute_IIT_accuracy(\"V2\", model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interchange Intervention Training (IIT)\n",
    "\n",
    "Original IIT [Geiger\\*, Wu\\*, Lu\\*, Rozner, Kreiss, Icard, Goodman, and Potts (2021)](https://arxiv.org/abs/2112.00826)\n",
    "\n",
    "IIT for model distillation [ Wu\\*,Geiger\\*, Rozner, Kreiss, Lu, Icard, Goodman, and Potts (2021)](https://arxiv.org/abs/2112.02505)\n",
    "\n",
    "Interchange intervention training is a method for training a neural network to conform to the causal structure of a high-level algorithm. Conceptually, it is a direct extension of the causal abstraction analysis we just performed, except instead of **evaluating** whether the neural network and algorithm produce the same outputs under aligned interchange interventions, we are now **training** the neural network to produce the output of the algorithm under aligned interchange interventions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 715. Training loss did not improve more than tol=1e-05. Final error is 0.0007661401759833097."
     ]
    }
   ],
   "source": [
    "V1 = 0\n",
    "V2 = 1\n",
    "both = 2\n",
    "id_to_coords = {V1:{1: [{\"layer\":1, \"start\":0, \"end\":embedding_dim}]}, \\\n",
    "    V2: {1: [{\"layer\":1, \"start\":embedding_dim, \"end\":embedding_dim*2}]}, \\\n",
    "    both: {1: [{\"layer\":1, \"start\":0, \"end\":embedding_dim},{\"layer\":1, \"start\":embedding_dim, \"end\":embedding_dim*2}]}}\n",
    "\n",
    "X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions = get_IIT_equality_dataset(\"V1\", embedding_dim ,data_size)\n",
    "\n",
    "model = TorchDeepNeuralClassifierIIT(hidden_dim=embedding_dim*4, hidden_activation=torch.nn.ReLU(), num_layers=3, id_to_coords=id_to_coords)\n",
    "_ = model.fit(X_base_train, X_sources_train, y_base_train, y_IIT_train,interventions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5120\n",
      "           1       1.00      1.00      1.00      5120\n",
      "\n",
      "    accuracy                           1.00     10240\n",
      "   macro avg       1.00      1.00      1.00     10240\n",
      "weighted avg       1.00      1.00      1.00     10240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5120\n",
      "           1       1.00      1.00      1.00      5120\n",
      "\n",
      "    accuracy                           1.00     10240\n",
      "   macro avg       1.00      1.00      1.00     10240\n",
      "weighted avg       1.00      1.00      1.00     10240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74      5120\n",
      "           1       0.74      0.73      0.74      5120\n",
      "\n",
      "    accuracy                           0.74     10240\n",
      "   macro avg       0.74      0.74      0.74     10240\n",
      "weighted avg       0.74      0.74      0.74     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_base_test, X_sources_test, y_base_test, y_IIT_test, interventions = get_IIT_equality_dataset(\"V1\", embedding_dim,data_size)\n",
    "\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_test, X_sources_test, interventions))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(classification_report(y_base_test, base_preds))\n",
    "print(classification_report(y_IIT_test, IIT_preds))\n",
    "\n",
    "\n",
    "X_base_test, X_sources_test, y_base_test, y_IIT_test, interventions = get_IIT_equality_dataset(\"V2\", embedding_dim,data_size)\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_test, X_sources_test, interventions))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(classification_report(y_IIT_test, IIT_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we now have perfect IIT accuracy **V1** meaning that under this alignment the neural network computes whether the first pair of inputs are equal. However, we still have low IIT accuracy for **V2**, meaning that under this alignment the neural network doesn't compute whether the second pair of inputs are equal.\n",
    "\n",
    "This is expected, because we only trained the network to compute **V1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the network to compute both **V1** and **V2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 584. Training loss did not improve more than tol=1e-05. Final error is 0.03112224835786037."
     ]
    }
   ],
   "source": [
    "model = TorchDeepNeuralClassifierIIT(hidden_dim=embedding_dim*4, hidden_activation=torch.nn.ReLU(), num_layers=3, id_to_coords=id_to_coords)\n",
    "\n",
    "\n",
    "v1data = get_IIT_equality_dataset(\"V1\", embedding_dim, data_size)\n",
    "v2data = get_IIT_equality_dataset(\"V2\", embedding_dim, data_size)\n",
    "X_base_train = torch.cat([v1data[0],v2data[0]], dim=0)\n",
    "X_sources_train = [ torch.cat([v1data[1][i],v2data[1][i]], dim=0) for i in range(len(v1data[1]))] \n",
    "y_base_train = torch.cat([v1data[2],v2data[2]])\n",
    "y_IIT_train = torch.cat([v1data[3],v2data[3]])\n",
    "interventions = torch.cat([v1data[4],v2data[4]])\n",
    "\n",
    "_ = model.fit(X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5120\n",
      "           1       1.00      1.00      1.00      5120\n",
      "\n",
      "    accuracy                           1.00     10240\n",
      "   macro avg       1.00      1.00      1.00     10240\n",
      "weighted avg       1.00      1.00      1.00     10240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5120\n",
      "           1       1.00      1.00      1.00      5120\n",
      "\n",
      "    accuracy                           1.00     10240\n",
      "   macro avg       1.00      1.00      1.00     10240\n",
      "weighted avg       1.00      1.00      1.00     10240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5120\n",
      "           1       1.00      1.00      1.00      5120\n",
      "\n",
      "    accuracy                           1.00     10240\n",
      "   macro avg       1.00      1.00      1.00     10240\n",
      "weighted avg       1.00      1.00      1.00     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_base_test, X_sources_test, y_base_test, y_IIT_test, interventions = get_IIT_equality_dataset(\"V1\", embedding_dim,data_size)\n",
    "\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_test, X_sources_test, interventions))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(classification_report(y_base_test, base_preds))\n",
    "print(classification_report(y_IIT_test, IIT_preds))\n",
    "\n",
    "\n",
    "X_base_test, X_sources_test, y_base_test, y_IIT_test, interventions = get_IIT_equality_dataset(\"V2\", embedding_dim,data_size)\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_test, X_sources_test, interventions))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(classification_report(y_IIT_test, IIT_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2916\n",
      "           1       1.00      1.00      1.00      3645\n",
      "\n",
      "    accuracy                           1.00      6561\n",
      "   macro avg       1.00      1.00      1.00      6561\n",
      "weighted avg       1.00      1.00      1.00      6561\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2916\n",
      "           1       1.00      1.00      1.00      3645\n",
      "\n",
      "    accuracy                           1.00      6561\n",
      "   macro avg       1.00      1.00      1.00      6561\n",
      "weighted avg       1.00      1.00      1.00      6561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(*compute_IIT_accuracy(\"V1\", model.model)))\n",
    "print(classification_report(*compute_IIT_accuracy(\"V2\", model.model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_multisource_interchange_A(base,source,source2):\n",
    "    return compute_A(base, {\"V1\":compute_A(source, {})[\"V1\"], \"V2\":compute_A(source2, {})[\"V2\"]})\n",
    "\n",
    "def multisource_interchange_intervention(model, base, sources, coords, output_coord):\n",
    "    source_activations = model.retrieve_activations(sources[0], coords[1][0],None)\n",
    "    source_activations2 = model.retrieve_activations(sources[1], coords[1][1],None)\n",
    "    coords = copy.deepcopy(coords)\n",
    "    coords[1][0][\"intervention\"] = source_activations\n",
    "    coords[1][1][\"intervention\"] = source_activations2\n",
    "    return model.retrieve_activations(base, output_coord, coords)\n",
    "\n",
    "def compute_multisource_IIT_accuracy(model, coords):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for base in itertools.product([pentagon, triangle, square], repeat=4):\n",
    "        for source in itertools.product([pentagon, triangle, square], repeat=4):\n",
    "            for source2 in itertools.product([pentagon, triangle, square], repeat=4):\n",
    "                basetensor = torch.cat([torch.tensor([base[k]]) for k in range(4)], 1)\n",
    "                sourcetensor = torch.cat([torch.tensor([source[k]]) for k in range(4)],1)\n",
    "                sourcetensor2 = torch.cat([torch.tensor([source2[k]]) for k in range(4)],1)\n",
    "                algorithm_output = compute_multisource_interchange_A(convert_input(basetensor, embedding_dim), convert_input(sourcetensor, embedding_dim),convert_input(sourcetensor2, embedding_dim))\n",
    "                if algorithm_output[\"output\"]:   \n",
    "                    labels.append(TRUE_LABEL)\n",
    "                else:\n",
    "                    labels.append(FALSE_LABEL)\n",
    "                get_coord = {\"layer\":3, \"start\":0, \"end\":2}\n",
    "                network_output = multisource_interchange_intervention(model, basetensor, [sourcetensor,sourcetensor2], coords, get_coord).argmax(axis=1)\n",
    "                predictions.append(int(network_output))\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    236196\n",
      "           1       1.00      1.00      1.00    295245\n",
      "\n",
      "    accuracy                           1.00    531441\n",
      "   macro avg       1.00      1.00      1.00    531441\n",
      "weighted avg       1.00      1.00      1.00    531441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sets = {1: [{\"layer\":1, \"start\":0, \"end\":embedding_dim},{\"layer\":1, \"start\":embedding_dim, \"end\":embedding_dim*2}]}\n",
    "print(classification_report(*compute_multisource_IIT_accuracy(model.model, sets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 440. Training loss did not improve more than tol=1e-05. Final error is 0.24750305037014186."
     ]
    }
   ],
   "source": [
    "v1data = get_IIT_equality_dataset(\"V1\", embedding_dim ,data_size)\n",
    "v2data = get_IIT_equality_dataset(\"V2\", embedding_dim ,data_size)\n",
    "bothdata = get_IIT_equality_dataset_both(embedding_dim ,data_size)\n",
    "X_base_train = torch.cat([v1data[0],v2data[0], bothdata[0]], dim=0)\n",
    "X_sources_train = [ torch.cat([v1data[1][0],v2data[1][0], bothdata[1][i]], dim=0) for i in range(len(bothdata[1]))] \n",
    "y_base_train = torch.cat([v1data[2],v2data[2],bothdata[2]])\n",
    "y_IIT_train = torch.cat([v1data[3],v2data[3], bothdata[3]])\n",
    "interventions = torch.cat([v1data[4],v2data[4], bothdata[4]])\n",
    "\n",
    "model = TorchDeepNeuralClassifierIIT(hidden_dim=embedding_dim*4, hidden_activation=torch.nn.ReLU(), num_layers=3, id_to_coords=id_to_coords)\n",
    "\n",
    "_ = model.fit(X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    236196\n",
      "           1       1.00      1.00      1.00    295245\n",
      "\n",
      "    accuracy                           1.00    531441\n",
      "   macro avg       1.00      1.00      1.00    531441\n",
      "weighted avg       1.00      1.00      1.00    531441\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2916\n",
      "           1       1.00      1.00      1.00      3645\n",
      "\n",
      "    accuracy                           1.00      6561\n",
      "   macro avg       1.00      1.00      1.00      6561\n",
      "weighted avg       1.00      1.00      1.00      6561\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2916\n",
      "           1       1.00      1.00      1.00      3645\n",
      "\n",
      "    accuracy                           1.00      6561\n",
      "   macro avg       1.00      1.00      1.00      6561\n",
      "weighted avg       1.00      1.00      1.00      6561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(*compute_multisource_IIT_accuracy(model.model, sets)))\n",
    "print(classification_report(*compute_IIT_accuracy(\"V1\", model.model)))\n",
    "print(classification_report(*compute_IIT_accuracy(\"V2\", model.model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9822e-02,  1.8774e-02,  1.5825e-05,  1.0034e-02, -3.4869e-02,\n",
      "          1.8639e-02,  1.1978e-02,  5.8974e-03, -1.0267e+00, -7.2273e-02,\n",
      "          8.2198e-01, -6.8227e-01,  1.0258e+00,  6.5848e-02, -8.2571e-01,\n",
      "          6.7370e-01],\n",
      "        [ 6.6098e-01, -5.8664e-01, -2.0231e-01,  1.1299e+00, -6.7887e-01,\n",
      "          5.6991e-01,  1.9586e-01, -1.1392e+00,  3.7295e-03,  2.1067e-02,\n",
      "          4.8160e-03, -9.9840e-03,  4.0230e-03, -1.3166e-02, -1.8038e-02,\n",
      "          2.0497e-02],\n",
      "        [ 4.8361e-03, -7.8354e-03,  7.4648e-03,  1.3511e-02,  1.1528e-02,\n",
      "          5.3422e-03,  2.0913e-02,  4.8559e-03,  8.6704e-01, -2.9921e-01,\n",
      "         -3.9933e-02, -9.7669e-01, -8.5531e-01,  2.8625e-01,  4.3487e-02,\n",
      "          9.7053e-01],\n",
      "        [ 1.2590e-01, -1.0640e-01,  2.2621e-01, -2.0776e-01, -1.9550e-01,\n",
      "          9.2362e-02, -2.2186e-01,  2.1222e-01, -5.3099e-01,  1.2107e-01,\n",
      "         -5.3057e-01,  6.3170e-02,  5.1720e-01, -1.4435e-01,  5.4054e-01,\n",
      "         -5.9443e-02],\n",
      "        [ 9.1320e-02,  1.3237e+00,  1.5307e-01, -2.5510e-01, -1.0804e-01,\n",
      "         -1.3053e+00, -1.4019e-01,  2.0348e-01, -1.1407e-02, -7.8059e-03,\n",
      "          1.3990e-02,  4.5411e-02, -2.1814e-02,  4.5148e-02,  7.4141e-03,\n",
      "          1.6821e-02],\n",
      "        [ 3.2759e-02, -2.8352e-02, -1.1133e-01,  6.7596e-02,  1.1150e-01,\n",
      "          1.0997e-01,  7.0117e-02, -4.5479e-02, -2.6820e-02,  3.0832e-01,\n",
      "         -7.1309e-01,  1.4719e-01, -3.7523e-02, -3.9311e-01,  7.8455e-01,\n",
      "         -7.9481e-02],\n",
      "        [ 5.3759e-03,  1.6574e-03, -2.5433e-03,  2.6251e-03,  1.1870e-02,\n",
      "          1.6432e-03, -1.0843e-02,  2.1308e-03,  5.9303e-01,  1.0955e+00,\n",
      "          4.6035e-01,  6.4948e-01, -5.9594e-01, -1.0981e+00, -4.5762e-01,\n",
      "         -6.4701e-01],\n",
      "        [-7.5748e-02,  1.3032e-02, -4.7496e-02,  1.2970e-01,  2.9621e-01,\n",
      "          3.3651e-01, -1.2235e-01, -3.8181e-01,  2.1078e-02, -4.2332e-01,\n",
      "          2.8728e-01,  1.3112e-01, -3.5308e-01, -5.1408e-02, -3.7981e-01,\n",
      "          2.7935e-01],\n",
      "        [-6.6080e-01, -2.4899e-01, -5.7694e-02, -1.7545e-01, -2.2099e-02,\n",
      "         -1.4900e-01,  6.2209e-01, -3.2479e-02, -2.2374e-01, -1.4965e-01,\n",
      "         -2.3647e-01,  3.0181e-01,  2.3765e-01,  1.9141e-01, -1.2353e-01,\n",
      "         -9.3258e-03],\n",
      "        [-4.1381e-01,  5.5124e-02, -6.0219e-01,  5.5020e-01,  3.9651e-01,\n",
      "         -7.5812e-02,  6.2560e-01, -5.5491e-01,  1.2416e-02,  9.1144e-03,\n",
      "         -1.8098e-02,  1.2906e-02,  4.3148e-04, -1.0635e-02,  7.2591e-03,\n",
      "          1.3180e-02],\n",
      "        [-2.0815e-02, -6.3840e-03, -2.8619e-03, -2.8614e-02,  3.8637e-02,\n",
      "          4.4090e-02, -3.5350e-02,  4.2515e-03,  7.7005e-02, -1.3124e+00,\n",
      "         -2.9602e-01,  8.7540e-01, -6.5000e-02,  1.3041e+00,  2.5207e-01,\n",
      "         -8.7526e-01],\n",
      "        [ 4.9355e-01, -1.0606e-01,  4.3127e-01, -7.9995e-01, -4.8732e-01,\n",
      "          1.1432e-01, -4.3877e-01,  7.6839e-01, -2.2575e-03, -1.9274e-02,\n",
      "         -2.1691e-02, -1.0981e-02,  1.3513e-02, -9.0759e-03, -2.3107e-04,\n",
      "          6.9192e-03],\n",
      "        [-1.0207e+00, -4.2870e-01,  8.4029e-01,  3.2910e-02,  1.0514e+00,\n",
      "          4.4687e-01, -8.6314e-01, -4.3460e-02,  3.9601e-02, -1.0944e-02,\n",
      "         -5.9957e-03, -2.0173e-02, -2.5904e-02,  4.8019e-03,  1.5041e-02,\n",
      "          4.1882e-02],\n",
      "        [-1.0964e-01,  1.2225e-01,  1.4239e-01, -6.7239e-02, -1.9488e-01,\n",
      "         -3.7660e-01, -1.2722e-02,  1.3045e-01,  1.7986e-01, -3.7284e-02,\n",
      "          1.3155e-01,  1.5603e-01, -4.2300e-01,  1.3639e-02, -2.3030e-01,\n",
      "         -2.6987e-01],\n",
      "        [ 1.1179e-01,  1.3237e-01, -2.9858e-02, -1.7910e-01,  4.0856e-01,\n",
      "          1.8853e-01, -5.3087e-03,  5.4722e-01,  2.1706e-01,  3.7747e-01,\n",
      "          7.3561e-01,  3.1161e-01, -1.6542e-01, -4.1061e-01, -6.7725e-01,\n",
      "         -2.4818e-01],\n",
      "        [-6.3666e-02, -2.3191e-01, -8.1149e-01, -8.0143e-01,  4.0984e-03,\n",
      "          2.1718e-01,  8.4763e-01,  7.7920e-01, -4.0145e-02, -1.1742e-02,\n",
      "         -3.8273e-02, -2.8199e-03,  4.0921e-02, -8.5440e-03,  1.3584e-02,\n",
      "          1.2836e-02]], device='cuda:0')\n",
      "tensor([-0.0205, -0.0171, -0.0264, -0.0234, -0.0111,  0.1084, -0.0104,  0.2731,\n",
      "        -0.2743, -0.0101, -0.0193,  0.0003,  0.0014,  0.2807,  0.0437, -0.0246],\n",
      "       device='cuda:0')\n",
      "tensor([[ 2.2335e-02, -2.4817e+00,  1.0593e-01,  3.8546e-02, -1.5715e+00,\n",
      "         -5.4494e-02,  1.4156e-01,  9.0126e-02,  6.5887e-01, -3.2245e+00,\n",
      "          7.9590e-02, -2.0626e+00, -2.3406e+00, -6.5623e-02, -1.0813e-01,\n",
      "         -2.2534e+00],\n",
      "        [ 6.0575e-01, -3.4797e-01, -1.2335e-01, -2.6240e-01, -3.3305e-02,\n",
      "         -8.5167e-02,  4.8573e-01,  3.0678e-01,  5.2154e-01,  4.5126e-01,\n",
      "          6.0267e-02,  2.8141e-01,  4.4330e-03,  2.6309e-01, -4.6142e-02,\n",
      "          1.2198e-01],\n",
      "        [ 1.1250e-02, -2.6142e+00,  3.7103e-02,  5.0247e-02, -1.9526e+00,\n",
      "          1.9299e-01,  1.2416e-01,  2.1969e-01,  8.2225e-01, -3.4498e+00,\n",
      "          6.6555e-02, -1.9478e+00, -2.1895e+00, -2.3543e-01, -1.6505e-02,\n",
      "         -3.0356e+00],\n",
      "        [-1.1312e-02,  5.2107e-01, -5.4535e-02, -5.1441e-02,  7.0609e-01,\n",
      "          2.4582e-01, -4.1682e-02,  4.2069e-02, -7.4420e-01,  2.3255e-01,\n",
      "          1.2089e-02,  4.2764e-01,  5.4019e-01,  4.2403e-01, -3.4045e-02,\n",
      "          6.1961e-01],\n",
      "        [ 2.3102e+00, -9.2211e-02,  2.6724e+00,  2.1867e-01,  6.3639e-02,\n",
      "          7.6309e-01,  1.1964e+00, -1.0015e-01,  1.0931e-01, -7.3597e-02,\n",
      "          2.1748e+00, -1.4201e-01,  6.9006e-03, -1.3347e-01, -5.3574e-01,\n",
      "         -2.9442e-05],\n",
      "        [-2.1814e+00, -2.9554e-01, -1.4577e+00, -1.2378e+00, -1.5739e-01,\n",
      "         -1.5547e-01, -2.4903e+00,  3.6686e-01,  2.4338e-02, -7.0307e-04,\n",
      "         -1.2225e+00,  2.4647e-01, -5.5790e-02,  1.1473e-01, -4.7169e-01,\n",
      "         -2.1627e-01],\n",
      "        [ 1.8460e+00,  7.7771e-02,  7.5004e-01,  6.3950e-01, -2.3657e-02,\n",
      "          7.5404e-01,  1.4385e+00, -1.3250e-01,  1.0587e-01,  2.3047e-03,\n",
      "          1.4704e+00,  1.2170e-02, -5.2359e-02,  6.9217e-02, -4.2246e-01,\n",
      "          7.6941e-02],\n",
      "        [-2.7102e+00,  3.1207e-01, -2.1674e+00, -1.0959e+00,  2.2979e-01,\n",
      "         -5.4150e-02, -7.9048e-01, -9.7547e-02,  4.3819e-02,  6.2084e-02,\n",
      "         -1.2269e+00,  5.5087e-01,  1.7008e-01, -2.0698e-01, -1.7615e-01,\n",
      "          2.8746e-01],\n",
      "        [-4.7844e-01, -1.3586e+00,  1.4959e-01, -1.2899e+00, -1.2762e-02,\n",
      "         -1.2521e-01, -1.2188e-01,  2.8022e-01,  1.5298e-01, -2.8527e-01,\n",
      "         -1.8316e+00, -1.7747e+00, -1.5365e+00, -1.4142e-01, -8.6999e-01,\n",
      "         -9.1010e-01],\n",
      "        [-7.4093e-01, -9.3316e-02,  5.1089e-02, -7.5744e-02, -7.4397e-01,\n",
      "         -1.1457e-01, -5.2836e-01, -3.2050e-01, -3.4760e-02, -1.3977e-01,\n",
      "         -7.0769e-01, -7.9293e-01, -7.5933e-02, -6.9406e-02,  3.5218e-01,\n",
      "         -1.4055e+00],\n",
      "        [ 5.0796e-01,  3.7296e-01, -2.1392e-01, -6.7622e-01,  3.2969e-02,\n",
      "          5.7114e-01,  5.7142e-03,  8.0728e-02,  4.2926e-01,  2.7954e-01,\n",
      "          3.9262e-01,  6.9682e-01,  5.0569e-01,  2.9395e-01, -3.2202e-01,\n",
      "          3.1574e-01],\n",
      "        [ 4.1236e-01,  1.4851e-01,  3.4380e-02,  2.1725e-01,  3.8976e-01,\n",
      "          8.3365e-02,  5.0059e-01, -6.9356e-02, -7.0520e-01, -1.0682e-01,\n",
      "         -3.6124e-01,  3.2378e-01,  3.0474e-01,  3.6726e-01,  2.9924e-02,\n",
      "          1.6967e-01],\n",
      "        [-5.3735e-01, -9.4310e-01, -1.0048e-02,  6.3078e-02, -7.1654e-02,\n",
      "          4.4434e-02, -2.2359e-01,  1.7818e-01, -3.2818e-01, -2.9431e-01,\n",
      "         -3.7154e-01, -5.9895e-01, -2.2050e-01,  4.6271e-01, -1.9682e-01,\n",
      "          7.5964e-04],\n",
      "        [ 8.8670e-02, -1.1301e-01, -9.6223e-02,  6.4346e-02, -1.7918e-01,\n",
      "          1.8166e-02, -2.6161e-02, -2.3552e-01, -2.1346e-01, -2.4545e-01,\n",
      "         -1.2748e-01,  8.2370e-02, -4.2876e-02, -5.4288e-02, -2.0027e-01,\n",
      "         -2.4683e-01],\n",
      "        [-1.1921e-01,  3.7745e-01, -3.6498e-01, -1.2101e-01,  5.1249e-03,\n",
      "          3.4740e-01, -1.2937e+00,  1.4116e-01, -5.7569e-02, -1.0117e-01,\n",
      "         -2.1148e+00,  3.1964e-01,  4.4954e-02, -3.1903e-01, -8.2368e-01,\n",
      "          2.7799e-01],\n",
      "        [ 1.2120e-01, -2.1893e+00,  9.3590e-02,  4.0537e-01, -1.2029e+00,\n",
      "          2.8205e-02,  1.5066e-01,  3.6791e-01, -2.4951e-01, -7.4712e-01,\n",
      "         -3.3650e-02, -8.8215e-01, -1.6564e+00,  8.5204e-02, -8.7943e-02,\n",
      "         -2.4350e-01]], device='cuda:0')\n",
      "tensor([ 0.3332,  0.7110,  0.3487,  0.1328,  0.1152,  0.2702,  0.0339,  0.4047,\n",
      "         0.1138,  0.1904, -0.1529, -0.1668,  0.1098, -0.2879,  0.0057, -0.1206],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.9459,  0.1529, -0.9281,  0.4135, -2.4428,  0.6720, -1.5392,  1.3148,\n",
      "         -0.6984, -0.0718, -0.3200, -0.0751,  0.3379, -0.2302,  0.2014, -0.2932],\n",
      "        [-1.0611,  0.3314, -1.0205,  0.1461, -2.0483,  0.0312, -1.4882,  0.4291,\n",
      "         -1.0441, -0.0677,  0.0710, -0.3828,  0.6161,  0.0736,  0.3119,  0.8852],\n",
      "        [-1.2501, -0.1577, -1.0996,  0.9487,  0.4795,  0.0658,  0.5094, -0.5181,\n",
      "          1.5577, -1.2339, -0.4023, -0.3581, -0.6918,  0.1286, -0.5122, -1.7858],\n",
      "        [-1.1640, -0.1596, -1.1002,  0.7453,  0.5186, -0.2038,  0.6705, -0.3874,\n",
      "          1.0662, -0.9829, -0.6325, -0.4357, -0.7862, -0.1595, -0.6591, -2.0227],\n",
      "        [-1.0671,  0.4668, -0.9420,  0.1826, -1.9503, -0.1527, -1.4351,  0.5538,\n",
      "         -1.4628, -0.1525, -0.0756, -0.8946,  0.6806, -0.1633,  0.5014,  0.4987],\n",
      "        [ 1.8083, -0.1820,  1.9619, -0.5927,  1.2433, -1.5528,  0.7501, -1.0945,\n",
      "         -1.3340,  0.4533,  0.4393, -0.0641,  1.0460, -0.0063,  0.3157,  1.8674],\n",
      "        [ 1.0431, -0.0556,  1.0521, -1.0717, -2.5653,  0.6945, -1.6103,  0.7901,\n",
      "          0.3295,  1.1950,  0.0101,  0.2880,  0.2098,  0.1272,  0.9270,  0.5458],\n",
      "        [ 0.8229, -0.0443,  0.8951, -1.4400, -2.4388,  1.2084, -1.1615,  1.2290,\n",
      "          0.2511,  0.9689, -0.0787,  0.1793, -0.0127, -0.2342,  0.7790,  0.5769],\n",
      "        [ 0.8970, -0.0211,  0.9324, -1.3132, -1.8700,  1.3999, -1.0459,  1.1444,\n",
      "          0.8863,  0.8981,  0.0426,  0.1387, -0.3986, -0.0405, -0.1378, -0.2696],\n",
      "        [-1.0686,  0.2720, -1.0830,  0.1735, -2.2729,  0.1504, -1.5135,  0.4867,\n",
      "         -1.5182, -0.1033,  0.1022, -0.4071,  0.5992, -0.1308,  0.2110,  0.6421],\n",
      "        [-1.0290,  0.3113, -1.2456,  0.5831, -2.5773,  0.9213, -1.4609,  1.2446,\n",
      "         -0.7085,  0.2173, -0.2522,  0.0570,  0.1825,  0.0495,  0.0365, -0.2329],\n",
      "        [-0.6328,  0.2062, -0.4390, -0.0330,  0.1405,  0.5867,  0.1149,  0.1124,\n",
      "          2.0473,  0.0508, -0.7596,  0.2050, -0.3387, -0.1051, -1.6984, -1.1889],\n",
      "        [ 0.7607,  0.0125,  0.8525, -1.1648, -1.9872,  1.1499, -1.0816,  1.0861,\n",
      "          0.6233,  0.8063, -0.0126,  0.1733,  0.0801, -0.0297,  0.1494,  0.0262],\n",
      "        [-2.2514,  0.6512, -1.9293,  0.7013,  0.3460, -1.0765,  0.8894, -0.8840,\n",
      "         -0.1187, -2.0816, -0.1930, -0.6113, -0.3286, -0.0456, -0.4234, -1.2738],\n",
      "        [-1.1724,  0.4763, -1.3479,  0.2013, -2.6395,  0.4038, -1.9243,  0.6385,\n",
      "         -1.2491, -0.2525, -0.0414, -0.2527,  0.3224, -0.1323,  0.2841,  0.2269],\n",
      "        [-0.0371, -0.2418,  0.2235, -0.1352,  0.0909,  0.1054, -0.0807, -0.2022,\n",
      "          0.1732, -0.2153, -0.2421,  0.2153,  0.0047, -0.1583,  0.0636, -0.0890]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.0262,  0.2232,  0.2518,  0.2387,  0.1325, -0.1625,  0.1792,  0.0784,\n",
      "        -0.0907,  0.2828, -0.1400,  0.3695, -0.0781,  0.1744,  0.2684, -0.2406],\n",
      "       device='cuda:0')\n",
      "tensor([[ 1.4020,  1.8501, -0.5574, -1.0239,  2.6928,  1.9815, -2.0031, -1.8672,\n",
      "         -1.8431,  1.7237,  1.5188, -0.7870, -1.9701, -1.9336,  1.6691, -0.1755],\n",
      "        [-1.6091, -2.1240,  0.8604,  1.2652, -2.2743, -1.9702,  1.7449,  1.8304,\n",
      "          1.7188, -1.7361, -1.2173,  0.9593,  1.8265,  1.8500, -1.5623,  0.0381]],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.0653, -0.0083], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for layer in model.model.layers:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        print(layer.state_dict()['weight'])\n",
    "        print(layer.state_dict()['bias'])\n",
    "    else:\n",
    "        print(layer.linear.state_dict()['weight'])\n",
    "        print(layer.linear.state_dict()['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "933b0a94e0d88ac80a17cb26ca3d8d36930c12815b02a2885c1925c2b1ae3c33"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
